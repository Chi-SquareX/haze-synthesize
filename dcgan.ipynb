{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T14:39:13.522659Z",
     "start_time": "2019-10-28T14:39:13.406139Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T14:43:48.025180Z",
     "start_time": "2019-10-28T14:43:47.987079Z"
    }
   },
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "#             if epoch % save_interval == 0:\n",
    "#                 self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T14:45:46.340067Z",
     "start_time": "2019-10-28T14:45:32.784955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.911208, acc.: 46.88%] [G loss: 0.747631]\n",
      "1 [D loss: 0.454905, acc.: 76.56%] [G loss: 0.797417]\n",
      "2 [D loss: 0.590017, acc.: 68.75%] [G loss: 0.977529]\n",
      "3 [D loss: 0.752588, acc.: 59.38%] [G loss: 1.200235]\n",
      "4 [D loss: 1.034200, acc.: 42.19%] [G loss: 1.314892]\n",
      "5 [D loss: 0.676268, acc.: 54.69%] [G loss: 1.327658]\n",
      "6 [D loss: 0.538324, acc.: 70.31%] [G loss: 1.167171]\n",
      "7 [D loss: 0.420474, acc.: 81.25%] [G loss: 1.342653]\n",
      "8 [D loss: 0.452096, acc.: 76.56%] [G loss: 0.911230]\n",
      "9 [D loss: 0.476495, acc.: 82.81%] [G loss: 1.124726]\n",
      "10 [D loss: 0.410108, acc.: 84.38%] [G loss: 1.239814]\n",
      "11 [D loss: 0.296878, acc.: 90.62%] [G loss: 1.216484]\n",
      "12 [D loss: 0.150035, acc.: 96.88%] [G loss: 1.110714]\n",
      "13 [D loss: 0.130148, acc.: 100.00%] [G loss: 1.273320]\n",
      "14 [D loss: 0.245734, acc.: 89.06%] [G loss: 1.197412]\n",
      "15 [D loss: 0.410219, acc.: 81.25%] [G loss: 1.704653]\n",
      "16 [D loss: 0.511895, acc.: 68.75%] [G loss: 2.534153]\n",
      "17 [D loss: 1.269351, acc.: 28.12%] [G loss: 2.219662]\n",
      "18 [D loss: 0.835095, acc.: 68.75%] [G loss: 1.953538]\n",
      "19 [D loss: 0.919188, acc.: 37.50%] [G loss: 1.331990]\n",
      "20 [D loss: 0.809148, acc.: 57.81%] [G loss: 1.245812]\n",
      "21 [D loss: 0.793525, acc.: 53.12%] [G loss: 0.894048]\n",
      "22 [D loss: 0.613660, acc.: 67.19%] [G loss: 1.030574]\n",
      "23 [D loss: 0.300805, acc.: 85.94%] [G loss: 1.002551]\n",
      "24 [D loss: 0.444567, acc.: 81.25%] [G loss: 1.154734]\n",
      "25 [D loss: 0.460555, acc.: 71.88%] [G loss: 1.086625]\n",
      "26 [D loss: 0.898871, acc.: 50.00%] [G loss: 1.372040]\n",
      "27 [D loss: 0.838293, acc.: 53.12%] [G loss: 1.690356]\n",
      "28 [D loss: 0.673952, acc.: 59.38%] [G loss: 1.905401]\n",
      "29 [D loss: 1.059104, acc.: 40.62%] [G loss: 1.177583]\n",
      "30 [D loss: 0.802985, acc.: 56.25%] [G loss: 1.123215]\n",
      "31 [D loss: 0.643975, acc.: 75.00%] [G loss: 1.349231]\n",
      "32 [D loss: 0.810682, acc.: 48.44%] [G loss: 1.391468]\n",
      "33 [D loss: 0.948306, acc.: 48.44%] [G loss: 1.401201]\n",
      "34 [D loss: 0.936936, acc.: 51.56%] [G loss: 1.391250]\n",
      "35 [D loss: 1.115817, acc.: 45.31%] [G loss: 1.201160]\n",
      "36 [D loss: 0.739614, acc.: 53.12%] [G loss: 1.025219]\n",
      "37 [D loss: 0.701332, acc.: 60.94%] [G loss: 1.166784]\n",
      "38 [D loss: 0.727041, acc.: 56.25%] [G loss: 1.054740]\n",
      "39 [D loss: 1.012313, acc.: 43.75%] [G loss: 1.016275]\n",
      "40 [D loss: 0.842298, acc.: 54.69%] [G loss: 0.952898]\n",
      "41 [D loss: 0.862992, acc.: 56.25%] [G loss: 1.233089]\n",
      "42 [D loss: 1.140021, acc.: 35.94%] [G loss: 1.357486]\n",
      "43 [D loss: 0.922978, acc.: 51.56%] [G loss: 1.314219]\n",
      "44 [D loss: 0.977798, acc.: 43.75%] [G loss: 1.218844]\n",
      "45 [D loss: 0.758466, acc.: 60.94%] [G loss: 1.490372]\n",
      "46 [D loss: 0.541677, acc.: 71.88%] [G loss: 1.382742]\n",
      "47 [D loss: 0.680218, acc.: 62.50%] [G loss: 1.588067]\n",
      "48 [D loss: 0.900754, acc.: 43.75%] [G loss: 1.616978]\n",
      "49 [D loss: 0.644620, acc.: 65.62%] [G loss: 1.297373]\n",
      "50 [D loss: 0.795823, acc.: 56.25%] [G loss: 1.239778]\n",
      "51 [D loss: 0.851451, acc.: 59.38%] [G loss: 1.315611]\n",
      "52 [D loss: 0.559628, acc.: 70.31%] [G loss: 1.278586]\n",
      "53 [D loss: 0.682670, acc.: 60.94%] [G loss: 1.678821]\n",
      "54 [D loss: 0.605255, acc.: 70.31%] [G loss: 1.668700]\n",
      "55 [D loss: 0.845372, acc.: 53.12%] [G loss: 1.268135]\n",
      "56 [D loss: 0.741350, acc.: 65.62%] [G loss: 1.467910]\n",
      "57 [D loss: 0.863159, acc.: 51.56%] [G loss: 1.470912]\n",
      "58 [D loss: 0.829855, acc.: 48.44%] [G loss: 1.325779]\n",
      "59 [D loss: 0.792909, acc.: 54.69%] [G loss: 1.579966]\n",
      "60 [D loss: 0.674610, acc.: 64.06%] [G loss: 1.475257]\n",
      "61 [D loss: 0.763319, acc.: 51.56%] [G loss: 1.362962]\n",
      "62 [D loss: 0.817516, acc.: 40.62%] [G loss: 1.192188]\n",
      "63 [D loss: 0.737404, acc.: 57.81%] [G loss: 1.301449]\n",
      "64 [D loss: 0.649626, acc.: 56.25%] [G loss: 1.471568]\n",
      "65 [D loss: 0.694208, acc.: 59.38%] [G loss: 1.505569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 [D loss: 0.960339, acc.: 43.75%] [G loss: 1.290011]\n",
      "67 [D loss: 0.775298, acc.: 59.38%] [G loss: 1.678275]\n",
      "68 [D loss: 0.913618, acc.: 43.75%] [G loss: 1.392920]\n",
      "69 [D loss: 1.062822, acc.: 34.38%] [G loss: 1.384826]\n",
      "70 [D loss: 0.792379, acc.: 56.25%] [G loss: 1.562794]\n",
      "71 [D loss: 0.800659, acc.: 57.81%] [G loss: 1.244223]\n",
      "72 [D loss: 0.799796, acc.: 50.00%] [G loss: 1.112647]\n",
      "73 [D loss: 0.913844, acc.: 50.00%] [G loss: 1.088915]\n",
      "74 [D loss: 0.901332, acc.: 50.00%] [G loss: 1.344868]\n",
      "75 [D loss: 0.714360, acc.: 65.62%] [G loss: 1.317540]\n",
      "76 [D loss: 0.959116, acc.: 48.44%] [G loss: 1.094937]\n",
      "77 [D loss: 0.711181, acc.: 64.06%] [G loss: 1.453285]\n",
      "78 [D loss: 0.735085, acc.: 59.38%] [G loss: 1.418275]\n",
      "79 [D loss: 0.881880, acc.: 48.44%] [G loss: 1.525971]\n",
      "80 [D loss: 0.934754, acc.: 46.88%] [G loss: 1.277141]\n",
      "81 [D loss: 0.877264, acc.: 51.56%] [G loss: 1.211108]\n",
      "82 [D loss: 0.934065, acc.: 48.44%] [G loss: 1.329385]\n",
      "83 [D loss: 0.729972, acc.: 54.69%] [G loss: 1.458241]\n",
      "84 [D loss: 1.012938, acc.: 43.75%] [G loss: 1.116526]\n",
      "85 [D loss: 0.855894, acc.: 53.12%] [G loss: 1.105142]\n",
      "86 [D loss: 0.738772, acc.: 60.94%] [G loss: 1.136145]\n",
      "87 [D loss: 0.788932, acc.: 59.38%] [G loss: 1.300030]\n",
      "88 [D loss: 0.818187, acc.: 57.81%] [G loss: 1.219343]\n",
      "89 [D loss: 0.847052, acc.: 53.12%] [G loss: 1.106259]\n",
      "90 [D loss: 0.868931, acc.: 56.25%] [G loss: 0.848606]\n",
      "91 [D loss: 0.908683, acc.: 53.12%] [G loss: 1.168004]\n",
      "92 [D loss: 0.803566, acc.: 56.25%] [G loss: 1.319977]\n",
      "93 [D loss: 0.924221, acc.: 45.31%] [G loss: 1.491876]\n",
      "94 [D loss: 1.048536, acc.: 35.94%] [G loss: 1.059777]\n",
      "95 [D loss: 0.775605, acc.: 59.38%] [G loss: 1.410886]\n",
      "96 [D loss: 1.037619, acc.: 35.94%] [G loss: 1.028516]\n",
      "97 [D loss: 0.734605, acc.: 59.38%] [G loss: 1.118521]\n",
      "98 [D loss: 0.652790, acc.: 59.38%] [G loss: 1.298773]\n",
      "99 [D loss: 0.760898, acc.: 54.69%] [G loss: 1.269521]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dcgan = DCGAN()\n",
    "    dcgan.train(epochs=100, batch_size=32, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
